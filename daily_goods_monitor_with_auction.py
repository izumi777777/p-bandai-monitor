import os
import time
import json
import logging
import re
import urllib.parse
import warnings
from datetime import datetime
from typing import Dict, List, Optional
from statistics import mean

# ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãƒãƒ³ãƒ€ã‚¤ã®Botå¯¾ç­–å›é¿ã‚¹ã‚¿ã‚¤ãƒ«ã«åˆã‚ã›ã€curl_cffi ã‚’ä½¿ç”¨
try:
    from curl_cffi import requests
except ImportError:
    import requests

from bs4 import BeautifulSoup
from dotenv import load_dotenv

# Firebase
import firebase_admin
from firebase_admin import credentials, firestore

# Azure AI Agent
from azure.ai.projects import AIProjectClient
from azure.identity import AzureCliCredential
from azure.ai.agents.models import ListSortOrder

# ========================================================
# 1. åˆæœŸè¨­å®šãƒ»ç’°å¢ƒå¤‰æ•°
# ========================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å†—é•·ãªãƒ­ã‚°ã‚’æŠ‘åˆ¶ (Azureã®HTTPãƒ­ã‚°ãªã©)
logging.getLogger("azure").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

# Firestoreã®è­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings("ignore", category=UserWarning, message="Detected filter using positional arguments")

logger = logging.getLogger(__name__)

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv(".env")

AZURE_PROJECT_ENDPOINT = os.getenv("AZURE_PROJECT_ENDPOINT")
AGENT_ID = os.getenv("AGENT_ID")
FIREBASE_KEY_PATH = os.getenv("FIREBASE_KEY_PATH", "service-account-key.json")
FIRESTORE_COLLECTION = "daily_goods_discontinued_monitor"
FIRESTORE_PURCHASE_LIST_COLLECTION = "daily_goods_purchase_list" # ä»•å…¥ã‚Œãƒªã‚¹ãƒˆç”¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³

# ç›£è¦–å¯¾è±¡ã®æ—¥ç”¨å“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
DAILY_GOODS_KEYWORDS = [
    "ã‚·ãƒ£ãƒ³ãƒ—ãƒ¼", "æ´—å‰¤", "æ­¯ç£¨ãç²‰", "åŒ–ç²§æ°´",
    "ä¹³æ¶²", "ãƒœãƒ‡ã‚£ã‚½ãƒ¼ãƒ—", "æŸ”è»Ÿå‰¤", "æ—¥ç”¨å“"
]

# è£½é€ çµ‚äº†æƒ…å ±ã®ã‚½ãƒ¼ã‚¹URLï¼ˆèŠ±ç‹ï¼‰
KAO_URL = "https://www.kao-kirei.com/ja/expire-item/khg/?tw=khg"

# èŠ±ç‹ã®ä¸»è¦ãƒ–ãƒ©ãƒ³ãƒ‰ãƒªã‚¹ãƒˆï¼ˆå•†å“ååˆ¤å®šç”¨ï¼‰
KAO_BRANDS = [
    "ãƒ“ã‚ªãƒ¬", "ãƒ‹ãƒ™ã‚¢", "ã‚¯ã‚¤ãƒƒã‚¯ãƒ«", "ã‚¢ã‚¿ãƒƒã‚¯", "ãƒãƒŸãƒ³ã‚°", "ãƒ¡ãƒªãƒ¼ã‚º", 
    "ãƒ­ãƒªã‚¨", "ã‚µã‚¯ã‚»ã‚¹", "ã‚±ãƒ¼ãƒ—", "ãƒªãƒ¼ã‚¼", "ã‚¨ãƒƒã‚»ãƒ³ã‚·ãƒ£ãƒ«", "ã‚»ã‚°ãƒ¬ã‚¿", 
    "ã‚­ãƒ¥ã‚­ãƒ¥ãƒƒãƒˆ", "ãƒã‚¸ãƒƒã‚¯ãƒªãƒ³", "ãƒã‚¤ã‚¿ãƒ¼", "ãƒªã‚»ãƒƒã‚·ãƒ¥", "ã‚¯ãƒªã‚¢ã‚¯ãƒªãƒ¼ãƒ³", 
    "ãƒ”ãƒ¥ã‚ªãƒ¼ãƒ©", "ãƒ‡ã‚£ãƒ¼ãƒ—ã‚¯ãƒªãƒ¼ãƒ³", "ãƒãƒ–", "ï¼˜ï½˜ï¼”", "ï¼˜Ã—ï¼”", "ines", "ã‚¤ãƒã‚¹", 
    "ã‚ãã‚Šã‚ºãƒ ", "ãƒ¯ã‚¤ãƒ‰ãƒã‚¤ã‚¿ãƒ¼", "ãƒ›ãƒ¼ãƒŸãƒ³ã‚°", "ãƒ•ã‚¡ãƒŸãƒªãƒ¼", "ã‚¨ãƒãƒ¼ãƒ«", 
    "ã‚¢ã‚¸ã‚¨ãƒ³ã‚¹", "ã‚¬ãƒ¼ãƒ‰ãƒãƒ­ãƒ¼", "ã‚¢ãƒˆãƒªãƒƒã‚¯ã‚¹", "IROKA", "ï¼©ï¼²ï¼¯ï¼«ï¼¡",
    "ãƒ–ãƒ­ãƒ¼ãƒ", "ãƒ—ãƒªãƒãƒ´ã‚£ã‚¹ã‚¿", "ã‚½ãƒ•ã‚£ãƒ¼ãƒŠ", "ã‚«ãƒãƒœã‚¦", "ã‚¢ãƒ«ãƒ–ãƒ©ãƒ³", "ã‚¨ã‚¹ãƒˆ"
]

# æŠ½å‡ºç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
DISCONTINUE_PATTERN = re.compile(r"(ç”Ÿç”£çµ‚äº†|çµ‚å£²|è²©å£²çµ‚äº†|åœ¨åº«åˆ‡ã‚Œ|ä¾›çµ¦åœæ­¢)")

# é™¤å¤–ãƒªã‚¹ãƒˆ
IGNORE_DOMAINS = ["youtube.com", "twitter.com", "x.com", "instagram.com"]
IGNORE_TEXTS = ["ãƒã‚¤ãƒšãƒ¼ã‚¸", "ãƒ­ã‚°ã‚¤ãƒ³", "ã‚«ãƒ¼ãƒˆ", "ãŠå•ã„åˆã‚ã›", "é–‰ã˜ã‚‹", "è©³ç´°ã¯ã“ã¡ã‚‰", "ã™ã¹ã¦", "ã”åˆ©ç”¨ã‚¬ã‚¤ãƒ‰", "ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰"]
IGNORE_TAGS = ["é™å®šå“", "åŒ»è–¬éƒ¨å¤–å“", "é™¤èŒ", "eco", "ä¼ç”»å“", "æŒ‡å®šåŒ»è–¬éƒ¨å¤–å“", "åŒ»è–¬è²»æ§é™¤å¯¾è±¡å“", "ã¤ã‚ã‹ãˆç”¨", "æœ¬ä½“"]

# ========================================================
# 2. Firebase / Azure AI Agent åˆæœŸåŒ–
# ========================================================
db = None
try:
    if not firebase_admin._apps:
        cred = credentials.Certificate(FIREBASE_KEY_PATH)
        firebase_admin.initialize_app(cred)
        logger.info("âœ… Firebase åˆæœŸåŒ–æˆåŠŸ")
    db = firestore.client()
except Exception as e:
    logger.error(f"âŒ Firebase åˆæœŸåŒ–å¤±æ•—: {e}")

def init_agent_client():
    try:
        project_client = AIProjectClient(
            credential=AzureCliCredential(),
            endpoint=AZURE_PROJECT_ENDPOINT
        )
        agent = project_client.agents.get_agent(AGENT_ID)
        logger.info("âœ… Azure AI Agent åˆæœŸåŒ–æˆåŠŸ")
        return project_client, agent
    except Exception as e:
        logger.error(f"âŒ Agent åˆæœŸåŒ–å¤±æ•—: {e}")
        return None, None

project_client, agent = init_agent_client()

# ========================================================
# 3. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ (å…±é€šå‡¦ç†)
# ========================================================
def fetch_text(url: str) -> str:
    """æŒ‡å®šURLã®æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆUser-Agentå½è£…ä»˜ãï¼‰"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    try:
        res = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "html.parser")
        for script in soup(["script", "style"]):
            script.decompose()
        text = soup.get_text(" ", strip=True)
        return text[:5000]
    except Exception:
        return ""

# ========================================================
# 4. ãƒ‡ãƒ¼ã‚¿åé›† (Source Gathering)
# ========================================================
def check_kao_website() -> List[Dict]:
    """èŠ±ç‹ã‚µã‚¤ãƒˆã‹ã‚‰è£½é€ çµ‚äº†å“ã‚’å–å¾—ï¼ˆã‚¿ã‚°åˆ†å‰²ãƒ»è¡Œå˜ä½è§£æç‰ˆï¼‰"""
    logger.info("ğŸ§´ èŠ±ç‹å…¬å¼ã‚µã‚¤ãƒˆè§£æé–‹å§‹...")
    try:
        res = requests.get(KAO_URL, impersonate="chrome120", timeout=15)
        if res.status_code != 200:
            logger.error(f"âŒ ã‚µã‚¤ãƒˆã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {res.status_code}")
            return []

        soup = BeautifulSoup(res.text, "html.parser")
        
        # ã€é‡è¦ã€‘separator='\n' ã‚’æŒ‡å®šã—ã¦ã€ã‚¿ã‚°ã®å¢ƒç•Œã§å¿…ãšæ”¹è¡Œã•ã›ã‚‹
        # ã“ã‚Œã«ã‚ˆã‚Šã€Œè£½é€ çµ‚äº†äºˆå®šå“ã€ã¨ã€Œå•†å“åã€ãŒé€£çµã•ã‚Œã‚‹ã®ã‚’é˜²ã
        all_text = soup.get_text(separator='\n', strip=True)
        
        # è¡Œã”ã¨ã«åˆ†å‰²ã—ã¦ãƒªã‚¹ãƒˆåŒ–
        lines = [line.strip() for line in all_text.split('\n') if line.strip()]

        items = []
        now = datetime.now()
        start_date = datetime(now.year - 1, now.month, 1) # 1å¹´å‰
        
        current_period = None
        is_period_valid = False
        
        i = 0
        while i < len(lines):
            line = lines[i]
            
            # --- 1. æ—¥ä»˜è¦‹å‡ºã—ã®åˆ¤å®š ---
            date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ', line)
            if date_match and len(line) < 20:
                year = int(date_match.group(1))
                month = int(date_match.group(2))
                try:
                    check_date = datetime(year, month, 1)
                    if check_date >= start_date:
                        current_period = f"{year}å¹´{month}æœˆ"
                        is_period_valid = True
                    else:
                        is_period_valid = False
                except ValueError:
                    is_period_valid = False
                i += 1
                continue

            if not is_period_valid:
                i += 1
                continue

            # --- 2. å•†å“æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ ---
            if "è£½é€ çµ‚äº†" in line:
                found_product = False
                for offset in range(1, 6):
                    if i + offset >= len(lines): break
                    candidate = lines[i + offset]
                    
                    if "è£½é€ çµ‚äº†" in candidate or re.search(r'\d{4}å¹´', candidate):
                        break
                    
                    if candidate in IGNORE_TAGS: continue
                    if any(ignore in candidate for ignore in IGNORE_TEXTS): continue
                    
                    is_category = False
                    if "ãƒ»" in candidate: is_category = True
                    if candidate.endswith(("æ´—å‰¤", "ãƒãƒ³ãƒ‰ã‚½ãƒ¼ãƒ—", "ã‚·ãƒ¼ãƒˆ", "ç”¨å“", "ã‚±ã‚¢", "ãƒã‚¹ã‚¯", "ã‚ªãƒ ãƒ„", "ãƒ‘ãƒƒãƒ‰", "å‰¤")):
                        if not any(brand in candidate for brand in KAO_BRANDS):
                            is_category = True
                    
                    if is_category:
                        continue

                    is_likely_product = False
                    if any(brand in candidate for brand in KAO_BRANDS):
                        is_likely_product = True
                    elif len(candidate) > 5:
                        is_likely_product = True

                    if is_likely_product:
                        name = candidate
                        name = re.sub(r'é™å®šå“|åŒ»è–¬éƒ¨å¤–å“|eco|ã¤ã‚ã‹ãˆç”¨|æœ¬ä½“|é™¤èŒ', '', name).strip()
                        
                        title = f"ã€èŠ±ç‹å…¬å¼ã€‘{name} ({current_period}çµ‚äº†)"
                        
                        if not any(item['title'] == title for item in items):
                            items.append({
                                "title": title,
                                "link": KAO_URL,
                                "pub_date": current_period,
                                "raw_name": name
                            })
                        found_product = True
                        break 
                
                if found_product:
                    pass
            
            i += 1

        logger.info(f"âœ… èŠ±ç‹ã‹ã‚‰ {len(items)} ä»¶ã®å¯¾è±¡å•†å“ã‚’æ¤œå‡º")
        return items

    except Exception as e:
        logger.error(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def get_google_news_topics(keyword: str) -> List[Dict]:
    """Googleãƒ‹ãƒ¥ãƒ¼ã‚¹RSSã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åˆè‡´ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
    encoded_query = urllib.parse.quote(keyword)
    url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ja&gl=JP&ceid=JP:ja"
    news_items = []
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.content, "xml")
        for item in soup.find_all("item"):
            title = item.title.text if item.title else ""
            if any(k in title for k in DAILY_GOODS_KEYWORDS):
                news_items.append({
                    "title": title,
                    "link": item.link.text if item.link else "",
                    "pub_date": item.pubDate.text if item.pubDate else ""
                })
            if len(news_items) >= 5: break
    except Exception as e:
        logger.error(f"âŒ Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
    return news_items

# ========================================================
# 5. è©³ç´°èª¿æŸ» (Market Enrichment)
# ========================================================
def get_yahoo_auction_stats(keyword: str) -> Dict:
    """ãƒ¤ãƒ•ã‚ªã‚¯ã§è½æœ­ç›¸å ´(closedsearch)ã‚’æ¤œç´¢ã—ã€ä¸Šä½5ä»¶ã®ä¾¡æ ¼çµ±è¨ˆã‚’å–å¾—"""
    # æ¤œç´¢ç”¨ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    clean_keyword = re.sub(r"ã€.*?ã€‘|\(.*?\)|è£½é€ çµ‚äº†.*?å“|é™å®šå“|åŒ»è–¬éƒ¨å¤–å“|æŒ‡å®šåŒ»è–¬éƒ¨å¤–å“|é™¤èŒ|eco", "", keyword)
    clean_keyword = re.sub(r"\d+(ml|mL|g|G|æš|å€‹)", "", clean_keyword)
    clean_keyword = clean_keyword.strip()

    if len(clean_keyword) < 2:
        return {"error": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸ååˆ†", "keyword": clean_keyword}

    logger.info(f"ğŸ’° ãƒ¤ãƒ•ã‚ªã‚¯è½æœ­ç›¸å ´ãƒã‚§ãƒƒã‚¯: {clean_keyword}")
    encoded = urllib.parse.quote_plus(clean_keyword)
    # è½æœ­ç›¸å ´URLã«å¤‰æ›´: closedsearch
    url = f"https://auctions.yahoo.co.jp/closedsearch/closedsearch?p={encoded}&b=1&n=50&mode=1"

    items_data = []
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # è½æœ­ç›¸å ´ãƒšãƒ¼ã‚¸ã§ã‚‚å•†å“ãƒªã‚¹ãƒˆã®ã‚¯ãƒ©ã‚¹åã¯æ¦‚ã­ "Product" ç³»
        products = soup.find_all("li", class_="Product")

        for product in products[:5]:
            title_tag = product.find("a", class_="Product__titleLink")
            if not title_tag: continue
            
            # è½æœ­ä¾¡æ ¼ (é–‹å‚¬ä¸­ã¨ã‚¯ãƒ©ã‚¹åã¯å…±é€šã®å ´åˆãŒå¤šã„ãŒã€å¿µã®ãŸã‚)
            price_tag = product.find("span", class_="Product__priceValue")
            price_str = price_tag.text.strip().replace("å††", "").replace(",", "") if price_tag else "0"
            try:
                price = int(float(price_str))
            except ValueError:
                price = 0

            items_data.append({"name": title_tag.text.strip(), "price": price})

    except Exception as e:
        logger.error(f"âŒ ãƒ¤ãƒ•ã‚ªã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return {"error": str(e)}

    prices = [i["price"] for i in items_data if i["price"] > 0]
    avg_price = int(mean(prices)) if prices else 0
    
    return {
        "keyword": clean_keyword,
        "total_hits": len(products),
        "avg_price": avg_price,
        "items": items_data
    }

# ========================================================
# 6. åˆ†æ (AI Analysis)
# ========================================================
def analyze_profit_margin(item: Dict, auction_data: Dict) -> Dict:
    """Azure AI Agentã«ã‚ˆã‚‹å®šä¾¡èª¿æŸ»ã¨åˆ©ã‚¶ãƒ¤åˆ¤å®š"""
    if not project_client or not agent: return {"error": "AgentæœªåˆæœŸåŒ–"}
    
    product_query = item.get('raw_name') or item['title']
    logger.info(f"ğŸ¤– åˆ©ã‚¶ãƒ¤åˆ†æé–‹å§‹: {product_query[:30]}")
    
    try:
        thread = project_client.agents.threads.create()
        prompt = f"""
ä»¥ä¸‹ã®è£½é€ çµ‚äº†å•†å“ã«ã¤ã„ã¦ã€Bing Searchã‚’ä½¿ç”¨ã—ã¦ã€Œå¸Œæœ›å°å£²ä¾¡æ ¼ï¼ˆå®šä¾¡ï¼‰ã€ã‚’èª¿æŸ»ã—ã€ãƒ¤ãƒ•ã‚ªã‚¯ç›¸å ´ã¨æ¯”è¼ƒã—ã¦ã€Œåˆ©ã‚¶ãƒ¤ã€ãŒå‡ºã‚‹ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

ã€èª¿æŸ»å¯¾è±¡ã€‘
å•†å“å: {product_query}
ãƒ¤ãƒ•ã‚ªã‚¯è½æœ­å¹³å‡ä¾¡æ ¼: {auction_data.get('avg_price', 0)}å††
ãƒ¤ãƒ•ã‚ªã‚¯è½æœ­ãƒ‡ãƒ¼ã‚¿æ•°: {auction_data.get('total_hits', 0)}ä»¶

ã€åˆ¤å®šæ‰‹é †ã€‘
1. Bing Searchã§ã“ã®å•†å“ã®æ­£ç¢ºãªã€Œå®šä¾¡(ç¨è¾¼)ã€ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚
2. å®šä¾¡ã¨ãƒ¤ãƒ•ã‚ªã‚¯è½æœ­å¹³å‡ä¾¡æ ¼ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚
3. è²©å£²æ‰‹æ•°æ–™(10%)ã¨é€æ–™ã‚’è€ƒæ…®ã—ã€åˆ©ç›ŠãŒå‡ºã‚‹ã‹è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

JSONã®ã¿ã§å‡ºåŠ›:
{{
  "product_name": "ç‰¹å®šã—ãŸæ­£å¼åç§°",
  "retail_price": "èª¿æŸ»ã—ãŸå®šä¾¡(å††)",
  "market_price": "ãƒ¤ãƒ•ã‚ªã‚¯è½æœ­å¹³å‡(å††)",
  "profit_margin": "æ¨å®šåˆ©ç›Šé¡(å††)",
  "judgment": "é«˜ / ä¸­ / ä½ / ãªã—",
  "analysis": "ç†ç”±ï¼ˆä¾‹ï¼šå®šä¾¡ã®2å€ã§å–å¼•ã•ã‚Œã¦ãŠã‚Šéœ€è¦éå¤šï¼‰"
}}
"""
        project_client.agents.messages.create(thread_id=thread.id, role="user", content=prompt)
        project_client.agents.runs.create_and_process(thread_id=thread.id, agent_id=agent.id)
        
        messages = project_client.agents.messages.list(thread_id=thread.id, order=ListSortOrder.DESCENDING)
        for m in messages:
            if m.role == "assistant" and m.text_messages:
                content = m.text_messages[0].text.value
                match = re.search(r"\{.*\}", content, re.DOTALL)
                if match: return json.loads(match.group())
        return {"error": "AIåˆ†æå¤±æ•—"}
    except Exception as e:
        return {"error": str(e)}

# ========================================================
# 7. ä¿å­˜ (Storage)
# ========================================================
def save_to_firestore(source: str, topic: Dict, report: Dict, auction_data: Dict):
    """çµæœã‚’ä¿å­˜"""
    if not db: return
    try:
        # 1. ç›£è¦–ãƒ­ã‚°ã¨ã—ã¦ä¿å­˜
        ref = db.collection(FIRESTORE_COLLECTION)
        existing = ref.where("title", "==", topic["title"]).limit(1).get()
        if list(existing): return

        doc = {
            "source": source,
            "title": topic["title"],
            "url": topic["link"],
            "analysis": report,
            "market_stats": {
                "avg_price": auction_data.get("avg_price"),
                "total_hits": auction_data.get("total_hits")
            },
            "created_at": datetime.utcnow()
        }
        ref.add(doc)
        logger.info(f"ğŸ’¾ ç›£è¦–ãƒ­ã‚°ä¿å­˜å®Œäº†: {topic['title'][:20]}")

        # 2. åˆ©ç›ŠãŒé«˜ã„å ´åˆã¯ã€Œä»•å…¥ã‚Œãƒªã‚¹ãƒˆã€ã«ã‚‚ä¿å­˜
        judgment = report.get("judgment", "")
        if judgment == "é«˜":
            purchase_ref = db.collection(FIRESTORE_PURCHASE_LIST_COLLECTION)
            # ä»•å…¥ã‚Œãƒªã‚¹ãƒˆå´ã§ã‚‚é‡è¤‡ãƒã‚§ãƒƒã‚¯
            existing_purchase = purchase_ref.where("title", "==", topic["title"]).limit(1).get()
            
            if not list(existing_purchase):
                purchase_doc = {
                    "source": source,
                    "title": topic["title"],
                    "product_name": report.get("product_name"),
                    "url": topic["link"],
                    "analysis": report,
                    "market_stats": {
                        "avg_price": auction_data.get("avg_price"),
                        "total_hits": auction_data.get("total_hits")
                    },
                    "profit_estimate": report.get("profit_margin"),
                    "status": "æœªä»•å…¥ã‚Œ", # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç®¡ç†ç”¨
                    "created_at": datetime.utcnow()
                }
                purchase_ref.add(purchase_doc)
                logger.info(f"ğŸ’° ä»•å…¥ã‚Œãƒªã‚¹ãƒˆã«è¿½åŠ : {topic['title'][:20]}")

    except Exception as e:
        logger.error(f"âŒ ä¿å­˜å¤±æ•—: {e}")

# ========================================================
# 8. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ (Main)
# ========================================================
def main():
    logger.info("ğŸš€ ç›£è¦–ãƒ»åˆ©ã‚¶ãƒ¤åˆ†æã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•")

    # 1. èŠ±ç‹å…¬å¼ã‚µã‚¤ãƒˆ
    kao_list = check_kao_website()
    total_kao = len(kao_list)
    logger.info(f"ğŸ“‹ èŠ±ç‹ãƒªã‚¹ãƒˆå–å¾—å®Œäº†: å…¨{total_kao}ä»¶")

    for i, item in enumerate(kao_list, 1):
        logger.info(f"â–¶ï¸ å‡¦ç†ä¸­ [{i}/{total_kao}]: {item['title'][:20]}...")
        
        # ç›¸å ´å–å¾— (è½æœ­ç›¸å ´)
        stats = get_yahoo_auction_stats(item['raw_name'])
        # AIåˆ†æ
        report = analyze_profit_margin(item, stats)
        # ä¿å­˜
        save_to_firestore("èŠ±ç‹å…¬å¼", item, report, stats)
        
        print(f"\nã€èŠ±ç‹ã€‘{report.get('product_name')}")
        print(f"å®šä¾¡: {report.get('retail_price')}å†† / ãƒ¤ãƒ•ã‚ªã‚¯è½æœ­å¹³å‡: {report.get('market_price')}å††")
        print(f"åˆ©ç›Šåˆ¤å®š: {report.get('judgment')} ({report.get('profit_margin')}å††)")
        time.sleep(2)

    # 2. Googleãƒ‹ãƒ¥ãƒ¼ã‚¹
    for keyword in DAILY_GOODS_KEYWORDS:
        query = f"{keyword} ç”Ÿç”£çµ‚äº† OR çµ‚å£²"
        news_list = get_google_news_topics(query)
        total_news = len(news_list)
        logger.info(f"ğŸ“‹ ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—å®Œäº† ({keyword}): å…¨{total_news}ä»¶")

        for i, topic in enumerate(news_list, 1):
            if any(d in topic["link"] for d in IGNORE_DOMAINS): continue
            
            logger.info(f"â–¶ï¸ å‡¦ç†ä¸­ [{i}/{total_news}]: {topic['title'][:20]}...")

            # æœ¬æ–‡å–å¾—
            body = fetch_text(topic["link"])
            if not DISCONTINUE_PATTERN.search(topic["title"] + body): continue

            # ç›¸å ´å–å¾— (è½æœ­ç›¸å ´)
            stats = get_yahoo_auction_stats(topic['title'])
            # AIåˆ†æ
            report = analyze_profit_margin(topic, stats)
            # ä¿å­˜
            save_to_firestore("ãƒ‹ãƒ¥ãƒ¼ã‚¹", topic, report, stats)
            
            print(f"\nã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€‘{topic['title'][:30]}")
            print(f"åˆ¤å®š: {report.get('judgment')}")
            time.sleep(2)

    logger.info("âœ… ç›£è¦–å®Œäº†")

if __name__ == "__main__":
    main()